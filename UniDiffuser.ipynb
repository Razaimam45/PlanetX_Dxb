{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vRJ-KSH5334"
      },
      "source": [
        "# [One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale](https://arxiv.org/abs/2303.06555)\n",
        "\n",
        "This is a demo for sampling from [UniDiffuser](https://arxiv.org/abs/2303.06555)  . UniDiffuser is a unified diffusion framework to fit all distributions relevant to a set of multi-modal data in one model. Implemented on large-scale paired image-text data, UniDiffuser is able to perform image, text, text-to-image, image-to-text, and image-text pair generation.\n",
        "\n",
        "[Paper](https://arxiv.org/abs/2303.06555) | [GitHub](https://github.com/thu-ml/unidiffuser)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEEqy9bRPYKY"
      },
      "source": [
        "# Dependency and Pretrained Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgAzXSsIA_wS"
      },
      "source": [
        "Download repository and install dependence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_txRP2VTAy3P"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/thu-ml/unidiffuser.git\n",
        "# !git clone https://github.com/openai/CLIP.git\n",
        "\n",
        "# !pip install -e ./CLIP\n",
        "# !pip install accelerate==0.12.0 absl-py ml_collections einops ftfy==6.1.1 transformers==4.23.1\n",
        "\n",
        "# !pip install -U xformers\n",
        "# !pip install -U --pre triton\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append('CLIP')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV-uS9KaJ8sw"
      },
      "source": [
        "Download pretrained models from HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7TqlfbVKFjC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('unidiffuser')\n",
        "\n",
        "# !mkdir models\n",
        "%cd models\n",
        "# !wget -c https://huggingface.co/thu-ml/unidiffuser-v1/resolve/main/autoencoder_kl.pth\n",
        "# !wget -c https://huggingface.co/thu-ml/unidiffuser-v1/resolve/main/caption_decoder.pth\n",
        "# !wget -c https://huggingface.co/thu-ml/unidiffuser-v1/resolve/main/uvit_v1.pth\n",
        "%cd ..\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUTfjc225L1o"
      },
      "source": [
        "Import what we need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE_YD_A_5OAq"
      },
      "outputs": [],
      "source": [
        "import ml_collections\n",
        "import torch\n",
        "import random\n",
        "import utils\n",
        "from dpm_solver_pp import NoiseScheduleVP, DPM_Solver\n",
        "from absl import logging\n",
        "import einops\n",
        "import libs.autoencoder\n",
        "import libs.clip\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import torchvision.transforms as standard_transforms\n",
        "import numpy as np\n",
        "from CLIP import clip\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yms6SlIC3qnK"
      },
      "source": [
        "Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGDO6IUI3qR2"
      },
      "outputs": [],
      "source": [
        "from libs.uvit_multi_post_ln_v1 import UViT\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "nnet = UViT(\n",
        "    img_size=64,\n",
        "    in_chans=4,\n",
        "    patch_size=2,\n",
        "    embed_dim=1536,\n",
        "    depth=30,\n",
        "    num_heads=24,\n",
        "    text_dim=64,\n",
        "    num_text_tokens=77,\n",
        "    clip_img_dim=512,\n",
        "    use_checkpoint=True\n",
        ")\n",
        "nnet.to(device)\n",
        "nnet.load_state_dict(torch.load('models/uvit_v1.pth', map_location='cpu'))\n",
        "nnet.eval()\n",
        "\n",
        "\n",
        "from libs.caption_decoder import CaptionDecoder\n",
        "caption_decoder = CaptionDecoder(device=device, pretrained_path=\"models/caption_decoder.pth\", hidden_dim=64)\n",
        "\n",
        "clip_text_model = libs.clip.FrozenCLIPEmbedder(device=device)\n",
        "clip_text_model.eval()\n",
        "clip_text_model.to(device)\n",
        "\n",
        "autoencoder = libs.autoencoder.get_model(pretrained_path='models/autoencoder_kl.pth')\n",
        "autoencoder.to(device)\n",
        "\n",
        "clip_img_model, clip_img_model_preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "\n",
        "@torch.cuda.amp.autocast()\n",
        "def encode(_batch):\n",
        "    return autoencoder.encode(_batch)\n",
        "\n",
        "@torch.cuda.amp.autocast()\n",
        "def decode(_batch):\n",
        "    return autoencoder.decode(_batch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr5JaWajQD3O"
      },
      "source": [
        "# Prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jia6mVVDGjYA"
      },
      "source": [
        "Define required function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9Ui064gGnlM"
      },
      "outputs": [],
      "source": [
        "def stable_diffusion_beta_schedule(linear_start=0.00085, linear_end=0.0120, n_timestep=1000):\n",
        "    _betas = (\n",
        "        torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2\n",
        "    )\n",
        "    return _betas.numpy()\n",
        "_betas = stable_diffusion_beta_schedule()\n",
        "N = len(_betas)\n",
        "\n",
        "def split(x):\n",
        "    C, H, W = 4, 64, 64\n",
        "    z_dim = C * H * W\n",
        "    z, clip_img = x.split([z_dim, 512], dim=1)\n",
        "    z = einops.rearrange(z, 'B (C H W) -> B C H W', C=C, H=H, W=W)\n",
        "    clip_img = einops.rearrange(clip_img, 'B (L D) -> B L D', L=1, D=512)\n",
        "    return z, clip_img\n",
        "\n",
        "def combine(z, clip_img):\n",
        "    z = einops.rearrange(z, 'B C H W -> B (C H W)')\n",
        "    clip_img = einops.rearrange(clip_img, 'B L D -> B (L D)')\n",
        "    return torch.concat([z, clip_img], dim=-1)\n",
        "\n",
        "def combine_joint(z, clip_img, text):\n",
        "    z = einops.rearrange(z, 'B C H W -> B (C H W)')\n",
        "    clip_img = einops.rearrange(clip_img, 'B L D -> B (L D)')\n",
        "    text = einops.rearrange(text, 'B L D -> B (L D)')\n",
        "    return torch.concat([z, clip_img, text], dim=-1)\n",
        "\n",
        "def split_joint(x):\n",
        "    C, H, W = 4, 64, 64\n",
        "    z_dim = C * H * W\n",
        "    z, clip_img, text = x.split([z_dim, 512, 77 * 64], dim=1)\n",
        "    z = einops.rearrange(z, 'B (C H W) -> B C H W', C=C, H=H, W=W)\n",
        "    clip_img = einops.rearrange(clip_img, 'B (L D) -> B L D', L=1, D=512)\n",
        "    text = einops.rearrange(text, 'B (L D) -> B L D', L=77, D=64)\n",
        "    return z, clip_img, text\n",
        "\n",
        "def unpreprocess(v):  # to B C H W and [0, 1]\n",
        "    v = 0.5 * (v + 1.)\n",
        "    v.clamp_(0., 1.)\n",
        "    return v\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def watermarking(save_path):\n",
        "    img_pre = Image.open(save_path)\n",
        "    img_pos = utils.add_water(img_pre)\n",
        "    img_pos.save(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDUqUaMMQL-0"
      },
      "source": [
        "# UniDiffuser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6optBAy9U12"
      },
      "source": [
        "Define the required functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQFyP5Vu9Ub5"
      },
      "outputs": [],
      "source": [
        "def t2i_nnet(x, timesteps, text):  # text is the low dimension version of the text clip embedding\n",
        "    \"\"\"\n",
        "    1. calculate the conditional model output\n",
        "    2. calculate unconditional model output\n",
        "        config.sample.t2i_cfg_mode == 'empty_token': using the original cfg with the empty string\n",
        "        config.sample.t2i_cfg_mode == 'true_uncond: using the unconditional model learned by our method\n",
        "    3. return linear combination of conditional output and unconditional output\n",
        "    \"\"\"\n",
        "    z, clip_img = split(x)\n",
        "\n",
        "    t_text = torch.zeros(timesteps.size(0), dtype=torch.int, device=device)\n",
        "\n",
        "    z_out, clip_img_out, text_out = nnet(z, clip_img, text=text, t_img=timesteps, t_text=t_text,\n",
        "                        data_type=torch.zeros_like(t_text, device=device, dtype=torch.int) + data_type)\n",
        "    x_out = combine(z_out, clip_img_out)\n",
        "\n",
        "    if cfg_scale == 0.:\n",
        "        return x_out\n",
        "\n",
        "    text_N = torch.randn_like(text)  # 3 other possible choices\n",
        "    z_out_uncond, clip_img_out_uncond, text_out_uncond = nnet(z, clip_img, text=text_N, t_img=timesteps, t_text=torch.ones_like(timesteps) * N,\n",
        "                                  data_type=torch.zeros_like(t_text, device=device, dtype=torch.int) + data_type)\n",
        "    x_out_uncond = combine(z_out_uncond, clip_img_out_uncond)\n",
        "\n",
        "\n",
        "    return x_out + cfg_scale * (x_out - x_out_uncond)\n",
        "\n",
        "def i_nnet(x, timesteps):\n",
        "    z, clip_img = split(x)\n",
        "    text = torch.randn(x.size(0), 77, 64, device=device)\n",
        "    t_text = torch.ones_like(timesteps) * N\n",
        "    z_out, clip_img_out, text_out = nnet(z, clip_img, text=text, t_img=timesteps, t_text=t_text,\n",
        "                          data_type=torch.zeros_like(t_text, device=device, dtype=torch.int) + data_type)\n",
        "    x_out = combine(z_out, clip_img_out)\n",
        "    return x_out\n",
        "\n",
        "def t_nnet(x, timesteps):\n",
        "    z = torch.randn(x.size(0), *[4, 64, 64], device=device)\n",
        "    clip_img = torch.randn(x.size(0), 1, 512, device=device)\n",
        "    z_out, clip_img_out, text_out = nnet(z, clip_img, text=x, t_img=torch.ones_like(timesteps) * N, t_text=timesteps,\n",
        "                        data_type=torch.zeros_like(timesteps, device=device, dtype=torch.int) + data_type)\n",
        "    return text_out\n",
        "\n",
        "def i2t_nnet(x, timesteps, z, clip_img):\n",
        "    \"\"\"\n",
        "    1. calculate the conditional model output\n",
        "    2. calculate unconditional model output\n",
        "    3. return linear combination of conditional output and unconditional output\n",
        "    \"\"\"\n",
        "    t_img = torch.zeros(timesteps.size(0), dtype=torch.int, device=device)\n",
        "\n",
        "    z_out, clip_img_out, text_out = nnet(z, clip_img, text=x, t_img=t_img, t_text=timesteps,\n",
        "                        data_type=torch.zeros_like(t_img, device=device, dtype=torch.int) + data_type)\n",
        "\n",
        "    if cfg_scale == 0.:\n",
        "        return text_out\n",
        "\n",
        "    z_N = torch.randn_like(z)  # 3 other possible choices\n",
        "    clip_img_N = torch.randn_like(clip_img)\n",
        "    z_out_uncond, clip_img_out_uncond, text_out_uncond = nnet(z_N, clip_img_N, text=x, t_img=torch.ones_like(timesteps) * N, t_text=timesteps,\n",
        "                                    data_type=torch.zeros_like(timesteps, device=device, dtype=torch.int) + data_type)\n",
        "\n",
        "    return text_out + cfg_scale * (text_out - text_out_uncond)\n",
        "\n",
        "def joint_nnet(x, timesteps):\n",
        "    z, clip_img, text = split_joint(x)\n",
        "    z_out, clip_img_out, text_out = nnet(z, clip_img, text=text, t_img=timesteps, t_text=timesteps,\n",
        "                        data_type=torch.zeros_like(timesteps, device=device, dtype=torch.int) + data_type)\n",
        "    x_out = combine_joint(z_out, clip_img_out, text_out)\n",
        "\n",
        "    if cfg_scale == 0.:\n",
        "        return x_out\n",
        "\n",
        "    z_noise = torch.randn(x.size(0), *(4, 64, 64), device=device)\n",
        "    clip_img_noise = torch.randn(x.size(0), 1, 512, device=device)\n",
        "    text_noise = torch.randn(x.size(0), 77, 64, device=device)\n",
        "\n",
        "    _, _, text_out_uncond = nnet(z_noise, clip_img_noise, text=text, t_img=torch.ones_like(timesteps) * N, t_text=timesteps,\n",
        "                      data_type=torch.zeros_like(timesteps, device=device, dtype=torch.int) + data_type)\n",
        "    z_out_uncond, clip_img_out_uncond, _ = nnet(z, clip_img, text=text_noise, t_img=timesteps, t_text=torch.ones_like(timesteps) * N,\n",
        "                            data_type=torch.zeros_like(timesteps, device=device, dtype=torch.int) + data_type)\n",
        "\n",
        "    x_out_uncond = combine_joint(z_out_uncond, clip_img_out_uncond, text_out_uncond)\n",
        "\n",
        "    return x_out + cfg_scale * (x_out - x_out_uncond)\n",
        "\n",
        "\n",
        "def sample_fn(mode, **kwargs):\n",
        "\n",
        "    _z_init = torch.randn(n_samples, *(4, 64, 64), device=device)\n",
        "    _clip_img_init = torch.randn(n_samples, 1, 512, device=device)\n",
        "    _text_init = torch.randn(n_samples, 77, 64, device=device)\n",
        "    if mode == 'joint':\n",
        "        _x_init = combine_joint(_z_init, _clip_img_init, _text_init)\n",
        "    elif mode in ['t2i', 'i']:\n",
        "        _x_init = combine(_z_init, _clip_img_init)\n",
        "    elif mode in ['i2t', 't']:\n",
        "        _x_init = _text_init\n",
        "    noise_schedule = NoiseScheduleVP(schedule='discrete', betas=torch.tensor(_betas, device=device).float())\n",
        "\n",
        "    def model_fn(x, t_continuous):\n",
        "        t = t_continuous * N\n",
        "        if mode == 'joint':\n",
        "            return joint_nnet(x, t)\n",
        "        elif mode == 't2i':\n",
        "            return t2i_nnet(x, t, **kwargs)\n",
        "        elif mode == 'i2t':\n",
        "            return i2t_nnet(x, t, **kwargs)\n",
        "        elif mode == 'i':\n",
        "            return i_nnet(x, t)\n",
        "        elif mode == 't':\n",
        "            return t_nnet(x, t)\n",
        "\n",
        "    dpm_solver = DPM_Solver(model_fn, noise_schedule, predict_x0=True, thresholding=False)\n",
        "    with torch.no_grad():\n",
        "        with torch.autocast(device_type=device):\n",
        "            x = dpm_solver.sample(_x_init, steps=steps, eps=1. / N, T=1.)\n",
        "\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    if mode == 'joint':\n",
        "        _z, _clip_img, _text = split_joint(x)\n",
        "        return _z, _clip_img, _text\n",
        "    elif mode in ['t2i', 'i']:\n",
        "        _z, _clip_img = split(x)\n",
        "        return _z, _clip_img\n",
        "    elif mode in ['i2t', 't']:\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_img_feature(image):\n",
        "    image = np.array(image).astype(np.uint8)\n",
        "    image = utils.center_crop(512, 512, image)\n",
        "    clip_img_feature = clip_img_model.encode_image(clip_img_model_preprocess(Image.fromarray(image)).unsqueeze(0).to(device))\n",
        "    image = (image / 127.5 - 1.0).astype(np.float32)\n",
        "    image = einops.rearrange(image, 'h w c -> 1 c h w')\n",
        "    image = torch.tensor(image, device=device)\n",
        "    moments = autoencoder.encode_moments(image)\n",
        "    return clip_img_feature, moments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV0uxXQ6CEoZ"
      },
      "source": [
        "Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "\n",
        "mode = 'i2t2i'\n",
        "\n",
        "# Define the root input directory containing all the classes\n",
        "input_root_directory = '/home/raza.imam/Documents/Spac/Spac/FG_dataset'\n",
        "output_root_directory = f'/home/raza.imam/Documents/Spac/Spac/Augmented_{mode}_dataset'\n",
        "n_samples = 2\n",
        "target_size = (512, 512)\n",
        "\n",
        "# Loop through the classes (subdirectories)\n",
        "for class_name in os.listdir(input_root_directory):\n",
        "    class_directory = os.path.join(input_root_directory, class_name)\n",
        "    print(class_directory)\n",
        "    # Skip non-directory entries\n",
        "    if not os.path.isdir(class_directory):\n",
        "        continue\n",
        "\n",
        "    # Create a corresponding output directory for the class\n",
        "    output_class_directory = os.path.join(output_root_directory, class_name)\n",
        "    os.makedirs(output_class_directory, exist_ok=True)\n",
        "\n",
        "    image_paths = [os.path.join(class_directory, filename) for filename in os.listdir(class_directory) if filename.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    for image_path in tqdm(image_paths):\n",
        "        assert mode in ['t2i', 'i2t', 'joint', 't', 'i', 't2i2t', 'i2t2i']\n",
        "        prompt = \"an elephant under the sea\"\n",
        "        img = image_path\n",
        "        seed = np.random.randint(0, 1000001)\n",
        "        steps = 50\n",
        "        cfg_scale = 8\n",
        "        n_samples = 4\n",
        "        nrow = 2\n",
        "        data_type = 1\n",
        "\n",
        "        if mode == 't2i' or mode == 't2i2t':\n",
        "            prompts = [prompt] * n_samples\n",
        "            contexts = clip_text_model.encode(prompts)\n",
        "            contexts_low_dim = caption_decoder.encode_prefix(contexts)\n",
        "        elif mode == 'i2t' or mode == 'i2t2i':\n",
        "            img_contexts = []\n",
        "            clip_imgs = []\n",
        "\n",
        "        image = Image.open(img).convert('RGB')\n",
        "        clip_img, img_context = get_img_feature(image)\n",
        "\n",
        "        img_contexts.append(img_context)\n",
        "        clip_imgs.append(clip_img)\n",
        "        img_contexts = img_contexts * n_samples\n",
        "        clip_imgs = clip_imgs * n_samples\n",
        "\n",
        "        img_contexts = torch.concat(img_contexts, dim=0)\n",
        "        z_img = autoencoder.sample(img_contexts)\n",
        "        clip_imgs = torch.stack(clip_imgs, dim=0)\n",
        "\n",
        "        set_seed(seed)\n",
        "        if mode == 't2i': #text-to-image generation\n",
        "            _z, _clip_img = sample_fn(mode, text=contexts_low_dim)  # conditioned on the text embedding\n",
        "        elif mode == 'i2t2i': #image-to-image generation\n",
        "            _text = sample_fn('i2t', z=z_img, clip_img=clip_imgs)  # conditioned on the image embedding\n",
        "            _z, _clip_img = sample_fn('t2i', text=_text)\n",
        "        samples = unpreprocess(decode(_z))\n",
        "        base_filename = os.path.splitext(os.path.basename(img))[0]\n",
        "\n",
        "        # Resize the input image\n",
        "        image = image.resize(target_size, Image.ANTIALIAS)\n",
        "\n",
        "        # Save the input image with its original filename to the class-specific output directory\n",
        "        input_image_output_path = os.path.join(output_class_directory, f'{base_filename}.png')\n",
        "        save_image(transforms.ToTensor()(image), input_image_output_path)\n",
        "\n",
        "        for idx, sample in enumerate(samples):\n",
        "            save_path = os.path.join(output_class_directory, f'{base_filename}_{idx}.png')\n",
        "            save_image(sample, save_path)\n",
        "\n",
        "print(\"Variations generation and saving completed.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
